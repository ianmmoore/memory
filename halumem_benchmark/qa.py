"""Question answering evaluation for HaluMem benchmark.

This module evaluates the ability of a memory system to correctly
answer questions using stored memories.
"""

import asyncio
import logging
from typing import Dict, List, Optional, Callable, Awaitable, Any
from dataclasses import dataclass

from .config import HaluMemConfig
from .dataset import HaluMemDataset, QAPair
from .metrics import QAMetrics, compute_qa_metrics

logger = logging.getLogger(__name__)


@dataclass
class QAResult:
    """Result of a QA evaluation.

    Attributes:
        qa_id: ID of the QA pair.
        question: The question asked.
        generated_answer: The answer generated by the system.
        ground_truth: The expected answer.
        correctness_score: Score from 0-1 indicating correctness.
        is_hallucination: Whether the answer contains hallucinated info.
        is_omission: Whether the answer is missing key information.
        retrieved_memory_ids: IDs of memories retrieved for this question.
        judge_reasoning: Explanation from the LLM judge.
    """
    qa_id: str
    question: str
    generated_answer: str
    ground_truth: str
    correctness_score: float = 0.0
    is_hallucination: bool = False
    is_omission: bool = False
    retrieved_memory_ids: List[str] = None
    judge_reasoning: str = ""

    def __post_init__(self):
        if self.retrieved_memory_ids is None:
            self.retrieved_memory_ids = []


class QAEvaluator:
    """Evaluates question answering capabilities.

    This class tests how well a memory system can answer questions
    using its stored memories.

    Example:
        >>> evaluator = QAEvaluator(config, dataset)
        >>> metrics = await evaluator.evaluate(
        ...     qa_fn=my_qa_function,
        ...     judge_fn=my_judge_function
        ... )
        >>> print(f"Correctness: {metrics.correctness:.2%}")
    """

    def __init__(
        self,
        config: HaluMemConfig,
        dataset: HaluMemDataset
    ):
        """Initialize the QA evaluator.

        Args:
            config: HaluMem configuration.
            dataset: Loaded HaluMem dataset.
        """
        self.config = config
        self.dataset = dataset

    async def answer_question(
        self,
        qa_pair: QAPair,
        qa_fn: Callable[[str], Awaitable[tuple[str, List[str]]]]
    ) -> tuple[str, List[str]]:
        """Get answer for a question using the QA function.

        Args:
            qa_pair: The QA pair to answer.
            qa_fn: Async function that takes a question and returns
                (answer, list_of_retrieved_memory_ids).

        Returns:
            Tuple of (answer, retrieved_memory_ids).
        """
        return await qa_fn(qa_pair.question)

    async def judge_answer(
        self,
        question: str,
        generated: str,
        ground_truth: str,
        judge_fn: Callable[[str, str, str], Awaitable[Dict[str, Any]]]
    ) -> Dict[str, Any]:
        """Judge the quality of a generated answer.

        Args:
            question: The question that was asked.
            generated: The generated answer.
            ground_truth: The expected answer.
            judge_fn: Async function that evaluates the answer.

        Returns:
            Dictionary with correctness_score, is_hallucination, is_omission, reasoning.
        """
        return await judge_fn(question, generated, ground_truth)

    async def evaluate_single(
        self,
        qa_pair: QAPair,
        qa_fn: Callable[[str], Awaitable[tuple[str, List[str]]]],
        judge_fn: Callable[[str, str, str], Awaitable[Dict[str, Any]]]
    ) -> QAResult:
        """Evaluate a single QA pair.

        Args:
            qa_pair: The QA pair to evaluate.
            qa_fn: Function to generate answers.
            judge_fn: Function to judge answers.

        Returns:
            QAResult with evaluation details.
        """
        # Generate answer
        answer, retrieved_ids = await self.answer_question(qa_pair, qa_fn)

        # Judge answer
        judgment = await self.judge_answer(
            qa_pair.question,
            answer,
            qa_pair.ground_truth_answer,
            judge_fn
        )

        return QAResult(
            qa_id=qa_pair.qa_id,
            question=qa_pair.question,
            generated_answer=answer,
            ground_truth=qa_pair.ground_truth_answer,
            correctness_score=judgment.get("correctness_score", 0.0),
            is_hallucination=judgment.get("is_hallucination", False),
            is_omission=judgment.get("is_omission", False),
            retrieved_memory_ids=retrieved_ids,
            judge_reasoning=judgment.get("reasoning", "")
        )

    async def evaluate(
        self,
        qa_fn: Callable[[str], Awaitable[tuple[str, List[str]]]],
        judge_fn: Callable[[str, str, str], Awaitable[Dict[str, Any]]],
        qa_ids: Optional[List[str]] = None
    ) -> QAMetrics:
        """Run QA evaluation on the dataset.

        Args:
            qa_fn: Async function that takes a question and returns
                (answer, list_of_retrieved_memory_ids).
            judge_fn: Async function that evaluates answer quality.
            qa_ids: Optional list of QA IDs to evaluate.
                If None, evaluates all (or sample_size if configured).

        Returns:
            QAMetrics with evaluation results.
        """
        qa_pairs = self.dataset.get_qa_pairs(limit=self.config.sample_size)

        if qa_ids:
            qa_pairs = [qa for qa in qa_pairs if qa.qa_id in qa_ids]

        if not qa_pairs:
            logger.warning("No QA pairs to evaluate")
            return QAMetrics()

        results = []
        batch_size = self.config.batch_size or 5  # Process 5 QA pairs in parallel to avoid rate limits

        async def evaluate_one(qa_pair) -> QAResult:
            """Evaluate a single QA pair."""
            return await self.evaluate_single(qa_pair, qa_fn, judge_fn)

        # Process in batches
        for batch_start in range(0, len(qa_pairs), batch_size):
            batch_end = min(batch_start + batch_size, len(qa_pairs))
            batch = qa_pairs[batch_start:batch_end]

            logger.info(f"Evaluating QA batch {batch_start//batch_size + 1}/{(len(qa_pairs) + batch_size - 1)//batch_size} (pairs {batch_start+1}-{batch_end}/{len(qa_pairs)})")

            # Run batch in parallel
            tasks = [evaluate_one(qa) for qa in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            for qa_pair, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.error(f"Error evaluating {qa_pair.qa_id}: {result}")
                    results.append(QAResult(
                        qa_id=qa_pair.qa_id,
                        question=qa_pair.question,
                        generated_answer="[ERROR]",
                        ground_truth=qa_pair.ground_truth_answer,
                        correctness_score=0.0,
                        is_hallucination=False,
                        is_omission=True,
                        judge_reasoning=f"Error: {str(result)}"
                    ))
                else:
                    results.append(result)
                    if result.correctness_score < 0.5:
                        logger.debug(
                            f"Low score for {qa_pair.qa_id}: "
                            f"score={result.correctness_score:.2f}, "
                            f"hallucination={result.is_hallucination}, "
                            f"omission={result.is_omission}"
                        )

            # Small delay between batches to avoid rate limiting
            if batch_end < len(qa_pairs):
                await asyncio.sleep(1.0)

        # Compute metrics
        correctness_scores = [r.correctness_score for r in results]
        hallucination_flags = [r.is_hallucination for r in results]
        omission_flags = [r.is_omission for r in results]
        question_types = [
            next((qa.question_type for qa in qa_pairs if qa.qa_id == r.qa_id), "unknown")
            for r in results
        ]

        metrics = compute_qa_metrics(
            correctness_scores=correctness_scores,
            hallucination_flags=hallucination_flags,
            omission_flags=omission_flags,
            question_types=question_types
        )

        logger.info(
            f"QA evaluation complete: Correctness={metrics.correctness:.2%}, "
            f"Hallucination={metrics.hallucination_rate:.2%}, "
            f"Omission={metrics.omission_rate:.2%}"
        )

        return metrics

    def get_detailed_results(self) -> List[QAResult]:
        """Get detailed results from the last evaluation."""
        # This would store results if we want to access them later
        return []


def create_memory_system_qa_fn(
    memory_system: Any  # MemorySystem
) -> Callable[[str], Awaitable[tuple[str, List[str]]]]:
    """Create a QA function that uses the memory system.

    Args:
        memory_system: The MemorySystem instance to use.

    Returns:
        Async function for answering questions.
    """
    async def qa_fn(question: str) -> tuple[str, List[str]]:
        # Retrieve relevant memories
        memories = await memory_system.retrieve_relevant_memories(question)
        memory_ids = [m.memory_id for m in memories]

        # Format memories for context
        memory_context = memory_system.format_memories_for_prompt(memories)

        # Generate answer using primary model
        prompt = f"""Based on the following memories, answer the question.

MEMORIES:
{memory_context}

QUESTION: {question}

ANSWER:"""

        # This would need the primary model function
        # For now, return a placeholder
        answer = "[Answer would be generated here]"

        return answer, memory_ids

    return qa_fn


def create_llm_judge_fn(
    model_fn: Callable[[str], Awaitable[str]]
) -> Callable[[str, str, str], Awaitable[Dict[str, Any]]]:
    """Create a judge function using an LLM.

    Args:
        model_fn: Async function to call the LLM.

    Returns:
        Async function that judges answer quality.
    """
    JUDGE_PROMPT = """You are an expert evaluator. Compare a generated answer to the ground truth answer for a question.

QUESTION: {question}

GENERATED ANSWER: {generated}

GROUND TRUTH ANSWER: {ground_truth}

Evaluate the generated answer on these criteria:
1. CORRECTNESS: How factually correct is the answer? (0.0 to 1.0)
2. HALLUCINATION: Does the answer contain information NOT supported by the ground truth?
3. OMISSION: Is the answer missing important information from the ground truth?

Respond in this exact format:
CORRECTNESS: <score from 0.0 to 1.0>
HALLUCINATION: <YES or NO>
OMISSION: <YES or NO>
REASONING: <brief explanation>"""

    async def judge_fn(
        question: str,
        generated: str,
        ground_truth: str
    ) -> Dict[str, Any]:
        prompt = JUDGE_PROMPT.format(
            question=question,
            generated=generated,
            ground_truth=ground_truth
        )
        response = await model_fn(prompt)

        # Parse response
        result = {
            "correctness_score": 0.0,
            "is_hallucination": False,
            "is_omission": False,
            "reasoning": ""
        }

        for line in response.strip().split("\n"):
            line = line.strip()
            if line.startswith("CORRECTNESS:"):
                try:
                    score = float(line[12:].strip())
                    result["correctness_score"] = max(0.0, min(1.0, score))
                except ValueError:
                    pass
            elif line.startswith("HALLUCINATION:"):
                result["is_hallucination"] = "YES" in line.upper()
            elif line.startswith("OMISSION:"):
                result["is_omission"] = "YES" in line.upper()
            elif line.startswith("REASONING:"):
                result["reasoning"] = line[10:].strip()

        return result

    return judge_fn
